---
title: "Walmart Sales Forcasting"
author: "Daniel Crofts"
format: pdf
editor: visual
---

## Libraries and Data

```{r}
library(tidyverse)
library(dplyr)
library(lubridate)
library(ggplot2)
library(discrim)
library(naivebayes)

dat_train <- read_csv("train.csv")
dat_test <- read_csv("test.csv")
features <- read_csv
glimpse(dat_train)

```

## EDA

### 1

```{r}
dat_train2 <- dat_train %>%
  mutate(Date = as.Date(Date),
         Week = isoweek(Date),
         Year = year(Date))

holiday_plot <- dat_train2 %>%
  group_by(Date, IsHoliday) %>%
  summarise(AvgSales = mean(Weekly_Sales), .groups = "drop")

ggplot(holiday_plot, aes(x = Date, y = AvgSales, color = IsHoliday)) +
  geom_line() +
  labs(title = "Average Weekly Sales Over Time (Holidays vs Non-Holidays)")


```

One issue we need to consider as shown in this graph is that holday weeks are weighted five times more heavily in this competition so that needs to be addressed in the model.

### 2

```{r}
store_var <- dat_train %>%
  group_by(Store) %>%
  summarise(AvgSales = mean(Weekly_Sales))


ggplot(store_var, aes(x = Store, y = AvgSales)) +
  geom_col() + labs(title = "Average Sales by Store")


```

This graph shows that there is significant variation in sales accross store locations. For our predictions to be accurate we will have to account for these differences at the store level.

### 3

```{r}
dat_train <- dat_train %>%
  mutate(
    Date = as.Date(Date),
    Year  = year(Date),
    Month = month(Date),
    Week  = isoweek(Date)
  )

trend <- dat_train %>%
  group_by(Year, Month) %>%
  summarise(AvgSales = mean(Weekly_Sales), .groups = "drop") %>%
  mutate(TimeIndex = (Year - min(Year)) * 12 + Month)

ggplot(trend, aes(x = TimeIndex, y = AvgSales)) +
  geom_line() +
  labs(title = "Monthly Average Walmart Sales Over Time",
       x = "Time (months from start)",
       y = "Average Weekly Sales")

```

This EDA shows that there is also significant variation in sales over time. We must consider time related patterns when predicting the data.

## Time Series Analysis

```{r}

train <- read_csv("train.csv")
test  <- read_csv("test.csv")
features <- read_csv("features.csv")
stores <- read_csv("stores.csv")

# 1


train <- train %>% mutate(Date = as.Date(Date))
test  <- test  %>% mutate(Date = as.Date(Date))
features <- features %>% mutate(Date = as.Date(Date))



features <- features %>%
  mutate(across(starts_with("MarkDown"), ~replace_na(., 0)))


features <- features %>%
  mutate(
    TotalMarkdown = MarkDown1 + MarkDown2 + MarkDown3 + MarkDown4 + MarkDown5,
    MarkdownFlag = if_else(TotalMarkdown > 0, 1, 0)
  )

# 2

train_full <- train %>%
  left_join(features, by = c("Store", "Date")) %>%
  left_join(stores,   by = "Store")

test_full <- test %>%
  left_join(features, by = c("Store", "Date")) %>%
  left_join(stores,   by = "Store")


glimpse(train_full)
glimpse(test_full)


```

## Naive Bayes

```{r}
dat_train <- read_csv("train.csv")
dat_test <- read_csv("test.csv")
cutoffs <- quantile(dat_train$Weekly_Sales, probs = seq(0, 1, by = 0.10))

# Create binned target and remove Weekly_Sales from predictors
train_model <- dat_train %>%
  mutate(
    SalesBin = cut(
      Weekly_Sales,
      breaks = cutoffs,
      include.lowest = TRUE,
      labels = FALSE
    ),
    SalesBin = factor(SalesBin)
  ) %>%
  dplyr::select(-Weekly_Sales)  # REMOVE the numeric target before recipe

# Test dataset already matches predictor columns
test_model <- dat_test

# ---------------------------------------------------------
# 2. RECIPE (NO Weekly_Sales anywhere)
# ---------------------------------------------------------

nb_recipe <- recipe(SalesBin ~ ., data = train_model) %>%
  update_role(Store, Dept, Date, new_role = "ID") %>%   # IDs, not predictors
  step_dummy(all_nominal_predictors()) %>%              # dummy-coding
  step_zv(all_predictors())                             # drop zero-var columns

# ---------------------------------------------------------
# 3. NAIVE BAYES MODEL SPEC
# ---------------------------------------------------------

nb_spec <- naive_Bayes() %>%
  set_engine("naivebayes") %>%
  set_mode("classification")

# ---------------------------------------------------------
# 4. WORKFLOW
# ---------------------------------------------------------

nb_wf <- workflow() %>%
  add_recipe(nb_recipe) %>%
  add_model(nb_spec)

# ---------------------------------------------------------
# 5. FIT THE MODEL
# ---------------------------------------------------------

nb_fit <- nb_wf %>% fit(data = train_model)

# ---------------------------------------------------------
# 6. PREDICT BINS FOR TEST DATA
# ---------------------------------------------------------

test_bins <- predict(nb_fit, test_model)

test_pred <- bind_cols(test_model, test_bins)

# ---------------------------------------------------------
# 7. CONVERT BIN PREDICTIONS BACK TO NUMERIC SALES
# ---------------------------------------------------------

bin_midpoints <- sapply(1:(length(cutoffs)-1), function(i) {
  mean(c(cutoffs[i], cutoffs[i+1]))
})

test_pred <- test_pred %>%
  mutate(
    Weekly_Sales = bin_midpoints[as.numeric(.pred_class)]
  )

# ---------------------------------------------------------
# 8. CREATE KAGGLE SUBMISSION FILE
# ---------------------------------------------------------

submission <- test_pred %>%
  mutate(Id = paste(Store, Dept, Date, sep = "_")) %>%
  dplyr::select(Id, Weekly_Sales)

write_csv(submission, "naive_bayes_submission.csv")
```

## Boosted Tree

```{r}
library(tidymodels)
library(xgboost)
library(finetune)
library(doParallel)


dat_train <- read_csv("train.csv")
dat_test <- read_csv("test.csv")

dat_train <- dat_train %>%
  mutate(
    Store = as.integer(Store),
    Dept  = as.integer(Dept),
    IsHoliday = as.integer(IsHoliday)
  )

dat_test <- dat_test %>%
  mutate(
    Store = as.integer(Store),
    Dept  = as.integer(Dept),
    IsHoliday = as.integer(IsHoliday)
  )


boost_recipe <- recipe(Weekly_Sales ~ ., data = dat_train) %>%
  step_date(Date, features = c("year", "month", "week", "decimal")) %>%
  step_rm(Date) %>%                 
  step_normalize(all_numeric_predictors())  

prep_rec <- prep(boost_recipe)

train_prepped <- bake(prep_rec, new_data = dat_train)
test_prepped  <- bake(prep_rec, new_data = dat_test)


X_train <- train_prepped %>%
  select(-Weekly_Sales) %>%
  mutate(across(everything(), as.numeric)) %>%
  as.matrix()

y_train <- train_prepped$Weekly_Sales

X_test <- test_prepped %>%
  mutate(across(everything(), as.numeric)) %>%
  as.matrix()


params <- list(
  objective = "reg:squarederror",
  eta = 0.05,   
  max_depth = 8,
  min_child_weight = 5,    
  subsample = 0.9,
  colsample_bytree = 0.8,
  nthread = 4
)

model <- xgboost(
  data = X_train,
  label = y_train,
  params = params,
  nrounds = 1500, 
  verbose = 1
)



preds <- predict(model, X_test)



submission <- dat_test %>%
  mutate(
    Id = paste(Store, Dept, Date, sep = "_"),
    Weekly_Sales = preds
  ) %>%
  select(Id, Weekly_Sales)

write_csv(submission, "xgb_submission.csv")

```

## Seasonal Naive (My best one)

```{r}
library(tidyverse)
library(lubridate)
library(forecast)
library(vroom)

dat_train <- vroom("train.csv")
dat_test  <- vroom("test.csv")

# Ensure dates are Date objects
dat_train <- dat_train %>% mutate(Date = as.Date(Date))
dat_test  <- dat_test %>% mutate(Date = as.Date(Date))

# Forecast horizon (should be 39 weeks)
h <- length(unique(dat_test$Date))



# Build forecasts by Store Ã— Dept


all_forecasts <- list()
pairs <- dat_train %>% distinct(Store, Dept)

for (i in seq_len(nrow(pairs))) {
  
  store_i <- pairs$Store[i]
  dept_i  <- pairs$Dept[i]
  
  message("Processing Store ", store_i, " Dept ", dept_i)
  
  df <- dat_train %>%
    filter(Store == store_i, Dept == dept_i) %>%
    arrange(Date)
  
  df_test <- dat_test %>%
    filter(Store == store_i, Dept == dept_i) %>%
    arrange(Date)
  
  # If there is no test data for this combo, skip it
  if (nrow(df_test) == 0) next
  
  # --- clean training series ---------------------------------
  y_raw <- df$Weekly_Sales
  
  # Drop rows where Weekly_Sales is NA
  good_idx <- !is.na(y_raw)
  y_raw <- y_raw[good_idx]
  
  # If < 2 good observations, just use overall mean from this dept/store
  if (length(y_raw) < 2) {
    fc <- rep(mean(df$Weekly_Sales, na.rm = TRUE), nrow(df_test))
    
  } else {
    # Replace any remaining NA (shouldn't be any after filter, but just in case)
    if (anyNA(y_raw)) {
      y_raw[is.na(y_raw)] <- mean(y_raw, na.rm = TRUE)
    }
    
    # Make weekly ts (frequency = 52)
    y <- ts(y_raw, frequency = 52)
    h <- nrow(df_test)
    
    # Model 1: seasonal naive
    
    fc_snaive <- tryCatch({
      fit_snaive <- snaive(y, h = h)
      as.numeric(fit_snaive$mean)
    }, error = function(e) {
      # fallback: simple mean if snaive fails
      rep(mean(y_raw), h)
    })
    
    # Model 2: stlf(ETS)
    
    fc_stlf <- tryCatch({
      fit_stlf <- stlf(y, h = h, method = "ets")
      as.numeric(fit_stlf$mean)
    }, error = function(e) {
      NULL  # if it fails, we'll ignore it
    })
    
    # Ensemble

    if (!is.null(fc_stlf)) {
      fc <- (fc_snaive + fc_stlf) / 2
    } else {
      fc <- fc_snaive
    }
  }
  
  all_forecasts[[i]] <- tibble(
    Store = store_i,
    Dept = dept_i,
    Date = df_test$Date,
    Weekly_Sales = as.numeric(fc)
  )
}

forecast_df <- bind_rows(all_forecasts)



# final submission file

submission <- submission %>%
  mutate(Id = paste(Store, Dept, Date, sep = "_")) %>%
  select(Id, Weekly_Sales)


vroom_write(submission, "seasonal_naive_submission.csv", delim = ',')


```